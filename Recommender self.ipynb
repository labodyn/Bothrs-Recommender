{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#########################################################################                                \n",
    "# Project: Vocal message recommender for the Bothrs startup based in Ghent\n",
    "# Author: Lander Bodyn\n",
    "# Date: 15/04/2017\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "df = pd.read_csv('Rating-Grid view.csv')\n",
    "\n",
    "# Build data matrix\n",
    "df2 = df.pivot(index='UserID', columns='MessageId', values='Rating')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>MessageId</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "MessageId   1    2    3    4    5    6    7    8    9    10   11   12   13\n",
       "UserID                                                                    \n",
       "1          1.0  1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  NaN  NaN  NaN\n",
       "2          1.0  1.0 -1.0 -1.0  1.0  1.0 -1.0 -1.0 -1.0 -1.0  NaN  NaN  NaN\n",
       "3          1.0  1.0  1.0 -1.0 -1.0 -1.0  1.0  1.0 -1.0 -1.0  1.0  1.0  1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2295524 ,  0.27593435,  0.05971963,  0.06136131,  0.24908662,\n",
       "         0.10517918,  0.17132224,  0.1232487 ,  0.18912612,  0.20358098,\n",
       "         0.08023294,  0.18565096,  0.20636378],\n",
       "       [ 0.15672323,  0.16376443,  0.04011238,  0.03833048,  0.15339052,\n",
       "         0.06797673,  0.1020582 ,  0.06308953,  0.09711285,  0.13878632,\n",
       "         0.0543136 ,  0.12895845,  0.14607698],\n",
       "       [ 0.09771064,  0.15145944,  0.02633184,  0.03103912,  0.12904486,\n",
       "         0.05006296,  0.09351336,  0.08153942,  0.1247066 ,  0.08693928,\n",
       "         0.0347927 ,  0.07597427,  0.08067954]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient descent parameters\n",
    "learning_rate = 0.1\n",
    "lambda_reg = 0.1  # Regularisation term\n",
    "init_size = 0.1\n",
    "\n",
    "# Number of features that will be learned. \n",
    "# This value needs to be crossvalidated\n",
    "n_features = 2 \n",
    "\n",
    "# \n",
    "n_users, n_messages = df2.shape\n",
    "\n",
    "init_size = 0.5\n",
    "x_init = (np.random.rand(n_features, n_messages) - 0.5)*init_size  # Uniform distribution\n",
    "theta_init = (np.random.rand(n_features, n_users) - 0.5)*init_size  # Uniform distribution\n",
    "\n",
    "def predictions(x, theta):\n",
    "    \"\"\" Returns the predictions using the movie and user features. \"\"\"\n",
    "    return np.dot(theta.T, x)\n",
    "\n",
    "predictions(x_init, theta_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-db11a57ed660>, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-db11a57ed660>\"\u001b[0;36m, line \u001b[0;32m23\u001b[0m\n\u001b[0;31m    cost =\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# We will implement the simple recommender system of Coursera machine learning course\n",
    "# https://www.coursera.org/learn/machine-learning/lecture/f26nH/collaborative-filtering-algorithm\n",
    "# This is content based recommender. \n",
    "# Works very well for reasonbly sized datasets.\n",
    "# But for new users and new messages: cold start problem\n",
    "# Can be solved with adding a item based recommender.\n",
    "# For very large datasets, more complex algorithms could be better. (Deep autoencoders?)\n",
    "\n",
    "# Gradient descent parameters\n",
    "learning_rate = 0.1\n",
    "lambda_reg = 0.1  # Regularisation term\n",
    "init_size = 0.1\n",
    "\n",
    "# Number of features that will be learned. \n",
    "# This value needs to be crossvalidated\n",
    "n_features = 2 \n",
    "\n",
    "# \n",
    "n_users, n_messages = df2.shape\n",
    "\n",
    "\n",
    "def cost_function(x, theta):\n",
    "    \"\"\" Function that returns the cost of the parameters \n",
    "    with the two partial derivatives\"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    cost_grad_x = np.zeros(n_features, n_messages)\n",
    "    cost_grad_th = = np.zeros(n_features, n_users) \n",
    "    \n",
    "    # Non-vectorized implementation. Hard to do because of nan-values which\n",
    "    # Should be skipped. For small datasets this is fine.\n",
    "    for k in range(n_features):\n",
    "        \n",
    "    \n",
    "    \n",
    "        # x grad\n",
    "        grad_x = []\n",
    "        for i, message in enumerate(df2):\n",
    "            grad = 0\n",
    "            for j, rating in enumerate(df2[message]):\n",
    "                if not np.isnan(rating):                \n",
    "                    grad += (np.inner(theta[:,j], x[:,i]) - rating)*theta[:,j] + lamda*x[:,i]\n",
    "            grad_x.append(grad)\n",
    "        cost_grad_x[k,:] = grad_x\n",
    "     \n",
    "                \n",
    "    return cost, cost_grad_x, cost_grad_th\n",
    "\n",
    "def gradient_descent():\n",
    "    # Initialise with random values for symmetry breaking\n",
    "    x_init = np.random.rand(n_features, n_messages)*init_size  # Uniform distribution\n",
    "    theta_init = np.random.rand(n_features, n_users)*init_size  # Uniform distribution\n",
    "\n",
    "    \n",
    "~                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'surprise'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-705976daaa8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_perf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'surprise'"
     ]
    }
   ],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import evaluate, print_perf\n",
    "\n",
    "\n",
    "# Load the movielens-100k dataset (download it if needed),\n",
    "# and split it into 3 folds for cross-validation.\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "data.split(n_folds=3)\n",
    "\n",
    "# We'll use the famous SVD algorithm.\n",
    "algo = SVD()\n",
    "\n",
    "# Evaluate performances of our algorithm on the dataset.\n",
    "perf = evaluate(algo, data, measures=['RMSE', 'MAE'])\n",
    "\n",
    "print_perf(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
